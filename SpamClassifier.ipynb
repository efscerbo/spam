{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "emails = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails.loc[:3999, :].to_csv('train_1.csv', index=False)\n",
    "emails.loc[4000:, :].to_csv('train_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: A&amp;L Daily to be auctioned in bankrupt...</td>\n",
       "      <td>URL: http://boingboing.net/#85534171\\n Date: N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: Wired: \"Stronger ties between ISPs an...</td>\n",
       "      <td>URL: http://scriptingnews.userland.com/backiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: It's just too small                  ...</td>\n",
       "      <td>&lt;HTML&gt;\\n &lt;HEAD&gt;\\n &lt;/HEAD&gt;\\n &lt;BODY&gt;\\n &lt;FONT SIZ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: liberal defnitions\\n</td>\n",
       "      <td>Depends on how much over spending vs. how much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: RE: [ILUG] Newbie seeks advice - Suse...</td>\n",
       "      <td>hehe sorry but if you hit caps lock twice the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: A&L Daily to be auctioned in bankrupt...   \n",
       "1   1  Subject: Wired: \"Stronger ties between ISPs an...   \n",
       "2   2  Subject: It's just too small                  ...   \n",
       "3   3                      Subject: liberal defnitions\\n   \n",
       "4   4  Subject: RE: [ILUG] Newbie seeks advice - Suse...   \n",
       "\n",
       "                                               email  spam  \n",
       "0  URL: http://boingboing.net/#85534171\\n Date: N...     0  \n",
       "1  URL: http://scriptingnews.userland.com/backiss...     0  \n",
       "2  <HTML>\\n <HEAD>\\n </HEAD>\\n <BODY>\\n <FONT SIZ...     1  \n",
       "3  Depends on how much over spending vs. how much...     0  \n",
       "4  hehe sorry but if you hit caps lock twice the ...     0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_1 = pd.read_csv('data/train_1.csv')\n",
    "emails_2 = pd.read_csv('data/train_2.csv')\n",
    "emails = pd.concat([emails_1, emails_2], ignore_index=True)\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape\n",
    "print(f'There are {emails.shape[0]} rows in the '\n",
    "      f'dataset and {emails.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "unique_PIDs = len(emails['id'].unique())\n",
    "total_PIDs = emails.shape[0]\n",
    "number_of_dupes = total_PIDs - unique_PIDs\n",
    "print(f'There are {number_of_dupes} duplicates in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split, seeded for replication\n",
    "train, test = train_test_split(emails, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "train.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance?\n",
    "train['spam'].replace({0: 'Ham', 1: 'Spam'}).value_counts() * 100 / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some emails have NaNs for their subjects\n",
    "def handle_missing_data(data):\n",
    "    data = data.fillna('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_upper(string):\n",
    "    \"\"\"Computes the fraction of alphabetical characters\n",
    "    in STRING that are uppercase. If no alphabetical\n",
    "    characters, returns 0.\"\"\"\n",
    "    num_upper = len(re.findall(r'[A-Z]', string))\n",
    "    num_letters = len(re.findall(r'[a-zA-Z]', string))\n",
    "    if num_letters == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num_upper / num_letters\n",
    "    \n",
    "def number_of(regex):\n",
    "    \"\"\"Returns a lambda that when applied to a string\n",
    "    will count the number of occurences of REGEX in the\n",
    "    string (for use in making new features below).\"\"\"\n",
    "    return lambda string: len(re.findall(regex, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_features(data):\n",
    "    # Combine subject and email columns\n",
    "    data['combined'] = data['subject'] + ' ' + data['email']\n",
    "\n",
    "    # Make everything in 'combined' lowercase and remove punctuation\n",
    "    data['no_punc'] = (\n",
    "        data['combined']\n",
    "        .str.lower()\n",
    "        .str.replace(pat=r'[^\\w\\s]', repl=' ')\n",
    "    )\n",
    "    \n",
    "    # Count number of characters, words, new line characters,\n",
    "    # etc. Take logs of these. Also compute fraction of\n",
    "    # uppercase letters in email\n",
    "    data['log_chars'] = np.log1p(data['combined'].apply(len))\n",
    "    data['log_words'] = np.log1p(data['no_punc'].apply(lambda string: len(string.split())))\n",
    "    data['log_new_lines'] = np.log1p(data['combined'].apply(number_of(r'[\\n]')))\n",
    "    data['log_angle_brackets'] = np.log1p(data['combined'].apply(number_of(r'[<>]')))\n",
    "    data['log_exclamations'] = np.log1p(data['combined'].apply(number_of(r'[!]')))\n",
    "    data['log_punctuation'] = np.log1p(data['combined'].apply(number_of(r'[\\n$%<>!?]')))\n",
    "    data['frac_upper'] = data['combined'].apply(frac_upper)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function runs only on 'train' so as to\n",
    "# guarantee that, when we run our data through the pipeline,\n",
    "# the same words are used for both 'train' and 'test'.\n",
    "def spam_ham_words(min_emails):\n",
    "    # Combine subject and email columns\n",
    "    train['combined'] = train['subject'] + ' ' + train['email']\n",
    "    \n",
    "    # Make everything in 'combined' lowercase and remove punctuation\n",
    "    train['no_punc'] = (\n",
    "        train['combined']\n",
    "        .str.lower()\n",
    "        .str.replace(pat=r'[^\\w\\s]', repl=' ')\n",
    "    )\n",
    "    \n",
    "    # Put email text into \"tidy format\", i.e., each word of each\n",
    "    # email gets put into its own row, indexed by id of email\n",
    "    tidy_format = (\n",
    "        train['no_punc']\n",
    "        .str.split(expand=True)\n",
    "        .stack()\n",
    "        .reset_index(level=1)\n",
    "        .rename(columns={'level_1': 'num', 0: 'word'})\n",
    "        # The following lines drop repeated words in same email\n",
    "        # Not sure if should keep these\n",
    "        #.drop('num', axis=1)\n",
    "        #.reset_index()\n",
    "        #.drop_duplicates()\n",
    "        #.set_index('index')\n",
    "    )\n",
    "    \n",
    "    # Find which words are most indicative of a spam email\n",
    "    # versus a ham email. Limit to words appearing in at\n",
    "    # least 'min_emails' emails\n",
    "    words = (\n",
    "        tidy_format\n",
    "        .groupby('word')\n",
    "        .filter(lambda x: x.index.nunique() >= min_emails)\n",
    "        .merge(train[['spam']], how=\"left\", left_index=True, right_index=True)\n",
    "        .groupby('word')[['spam']]\n",
    "        .mean()\n",
    "        .sort_values('spam', ascending=False)\n",
    "    )\n",
    "    \n",
    "    return words\n",
    "\n",
    "def words_in_texts(words, texts):\n",
    "    \"\"\"Returns a dataframe the (i, j)^th entry of which is 1 \n",
    "    if the i^th element of TEXTS contains the j^th element of\n",
    "    WORDS as a substring and is 0 otherwise.\"\"\"\n",
    "    indicator_array = np.array([texts.str.contains(word).astype(int) for word in words]).T\n",
    "    df = pd.DataFrame(indicator_array, columns=words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_words_cols(data, words):\n",
    "    \"\"\"For each word in WORDS, appends a binary feature \n",
    "    indicating whether that word appears in a given email.\"\"\"\n",
    "    # Must reset index on 'data' for use in concat\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    words_in_texts_df = words_in_texts(words, data['no_punc'])\n",
    "    data = pd.concat([data, words_in_texts_df], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "def drop_cols(data, cols):\n",
    "    data = data.drop(cols, axis=1)\n",
    "    return data\n",
    "\n",
    "cols_to_drop = ['id', 'subject', 'email', 'combined', 'no_punc']\n",
    "#cols_to_drop = ['id', 'subject', 'email', 'combined', 'no_punc', 'log_angle_brackets', 'log_exclamations']\n",
    "\n",
    "# The graphs for 'log_angle_brackets' and 'log_exclamations'\n",
    "# are strange. Not clear whether we should include these\n",
    "# features. We'll try running the model with them and\n",
    "# try it again without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_pipeline(data):\n",
    "    data = (\n",
    "        data\n",
    "        .pipe(handle_missing_data)\n",
    "        .pipe(make_new_features)\n",
    "        .pipe(append_words_cols, words)\n",
    "        .pipe(drop_cols, cols_to_drop)\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider using LogisticRegressionCV\n",
    "#### Also, hyperparams to tweak: 'min_emails', 'num_spam', and 'num_ham' in 'spam_ham_words', as well as classification threshold in LogisticRegression model object. Also consider dropping the 'log_angle_brackets' and 'log_exclamations' cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "#\n",
    "# To do this \"right\", probably want to use KFold and\n",
    "# split the training set. Also, will probably want to\n",
    "# optimize recall rather than accuracy; see below\n",
    "\n",
    "import itertools\n",
    "\n",
    "min_emails_range = range(600, 1201, 100)\n",
    "num_spam_range = range(30, 101, 15)\n",
    "num_ham_range = range(30, 101, 15)\n",
    "\n",
    "training_scores = {}\n",
    "test_scores = {}\n",
    "\n",
    "# Use nested 'for' loops here b/c computing 'words' is\n",
    "# the most expensive part. No sense recomputing 'words'\n",
    "# multiple times w the same value of 'min_emails'\n",
    "for min_emails in min_emails_range:\n",
    "    words_df = spam_ham_words(min_emails)\n",
    "    for num_spam, num_ham in itertools.product(num_spam_range, num_ham_range):\n",
    "        spam_words = words_df.index[:num_spam].tolist()\n",
    "        ham_words = words_df.index[-num_ham:].tolist()\n",
    "        words = spam_words + ham_words\n",
    "        \n",
    "        processed_train = data_process_pipeline(train.copy().rename(columns={'spam': 'Spam'}))\n",
    "        processed_test = data_process_pipeline(test.copy().rename(columns={'spam': 'Spam'}))\n",
    "        \n",
    "        X_train = processed_train.drop('Spam', axis=1)\n",
    "        y_train = processed_train['Spam']\n",
    "        X_test = processed_test.drop('Spam', axis=1)\n",
    "        y_test = processed_test['Spam']\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        for C in np.linspace(0, 1, 100):\n",
    "            y_pred = 1*(model.predict_proba(X_train)[:, 1] > C)\n",
    "            # May want to track a different metric here instead of\n",
    "            # accuracy, maybe precision, recall or f1. Look into this\n",
    "            score = sum(y_pred == y_train) / len(y_train)\n",
    "            scores[C] = score\n",
    "        \n",
    "        C = max(scores, key=scores.get)\n",
    "        \n",
    "        y_train_pred = 1*(model.predict_proba(X_train)[:, 1] > C)\n",
    "        training_accuracy = sum(y_train_pred == y_train) / len(y_train)\n",
    "        \n",
    "        y_test_pred = 1*(model.predict_proba(X_test)[:, 1] > C)\n",
    "        test_accuracy = sum(y_test_pred == y_test) / len(y_test)\n",
    "        \n",
    "        quadruple = (min_emails, num_spam, num_ham, C)\n",
    "        training_scores[quadruple] = training_accuracy\n",
    "        test_scores[quadruple] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_emails, num_spam, num_ham, C = max(test_scores, key=test_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(training_scores, key=training_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = spam_ham_words(min_emails)\n",
    "spam_words = words_df.index[:num_spam].tolist()\n",
    "ham_words = words_df.index[-num_ham:].tolist()\n",
    "words = spam_words + ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframes before modeling. Rename 'spam' as\n",
    "# 'Spam' bc one of the words found by 'spam_ham_words'\n",
    "# is 'spam'\n",
    "processed_train = data_process_pipeline(train.copy().rename(columns={'spam': 'Spam'}))\n",
    "processed_test = data_process_pipeline(test.copy().rename(columns={'spam': 'Spam'}))\n",
    "\n",
    "X_train = processed_train.drop('Spam', axis=1)\n",
    "y_train = processed_train['Spam']\n",
    "X_test = processed_test.drop('Spam', axis=1)\n",
    "y_test = processed_test['Spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = 1*(model.predict_proba(X_train)[:, 1] > C)\n",
    "training_accuracy = sum(y_train_pred == y_train) / len(y_train)\n",
    "        \n",
    "y_test_pred = 1*(model.predict_proba(X_test)[:, 1] > C)\n",
    "test_accuracy = sum(y_test_pred == y_test) / len(y_test)\n",
    "\n",
    "#training_accuracy = model.score(X_train, y_train)\n",
    "#test_accuracy = model.score(X_test, y_test)\n",
    "print('Training Accuracy: ', training_accuracy)\n",
    "print('Test Accuracy: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict_proba(X_train)[:, 1]\n",
    "prec, recall, _ = precision_recall_curve(y_train, y_predict)\n",
    "plt.plot(recall, prec)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "evaluation = pd.read_csv('data/eval.csv')\n",
    "evaluation_predictions = model.predict(data_process_pipeline(evaluation.copy()))\n",
    "\n",
    "# Construct and save the submission:\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": evaluation['id'], \n",
    "    \"Class\": evaluation_predictions,\n",
    "}, columns=['Id', 'Class'])\n",
    "timestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\n",
    "submission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_words = (\n",
    "    {'body', 'click', 'please', 'base64', '2002', 'html', 'subscribed', \n",
    "     'wrote', 'mortgage', 'align3dcenterfont', 'dear', 'br', 'width10img',\n",
    "     'divfont', 'im', 'receive', 'list', 'tags', 'web', 'click',\n",
    "     'body', 'please', 'money', 'offer', 'receive', 'contact', 'free',\n",
    "     'tr', 'removed', 'remove', 'html', 'font', 'form',\n",
    "     'credit', 'business', 'div'}\n",
    ")\n",
    "\n",
    "# Lists of words to check (back from when I was working on this in March):\n",
    "words_to_check = (\n",
    "    {'body', 'business', 'html', 'money', 'offer', 'please',\n",
    "     'click', 'please', '2002', 'html', 'subscribed', 're:',\n",
    "     'wrote', 'mortgage', 'dear', 'br', 'receive', 'list', 'fwd',\n",
    "     'web', 'money', 'offer', 'contact', 'free', 'tr', 'removed', \n",
    "     'remove', 'font', 'form', 'credit', 'business', 'div', 'small',\n",
    "     'drug', 'bank', 'prescription', 'memo', 'private', 'selected', \n",
    "     'viagra', 'large', 'penis', 'horny', 'login', 'cash', 'loan', \n",
    "     'now', ' hi', 'act', 'limited', 'sex', 'today', 'free', \n",
    "     'gift', 'adult', 'member', 'buy', 'time', 'access', 'password', \n",
    "     'member', 'celeb', 'porn', 'remov', 'click', 'wealth', 'name',\n",
    "     'address', 'work', '---', 'girl', 'babe', 'xxx', 'subscri',\n",
    "     'weight', '...', 'align', 'font', '==', 'niger', 'zimb', \n",
    "     'invest', 'spam', 'original', 'repl', 'virus', 'url', 'wrote'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_format = (\n",
    "    train['no_punc']\n",
    "    .str.split(expand=True)\n",
    "    .stack()\n",
    "    .reset_index(level=1)\n",
    "    .rename(columns={'level_1': 'num', 0: 'word'})\n",
    ")\n",
    "\n",
    "counts = (\n",
    "    tidy_format\n",
    "    .groupby('word')\n",
    "    .agg(lambda x: x.index.nunique())\n",
    "    .sort_values('num', ascending=False)\n",
    ")\n",
    "\n",
    "words = (\n",
    "    tidy_format\n",
    "    .groupby('word')\n",
    "    .filter(lambda x: x.index.nunique() >= 400)\n",
    "    .merge(train[['spam']], how=\"left\", left_index=True, right_index=True)\n",
    "    .groupby('word')[['spam']]\n",
    "    .mean()\n",
    "    .sort_values('spam', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_check = sorted([word for word in staff_words if word in words.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.loc[words_to_check].sort_values('spam', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
